{
  "metadata": {
    "name": "Crime report",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#df_crime \u003d spark.read.csv(\"hdfs:///user/ubuntu/crime.csv\")\n#df_crime.show(10)\n\ndf_crime \u003d spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv(\"hdfs:///user/ubuntu/crime.csv\")\ndf_crime.show(10)\nspark.version\n\n# create DataFrame from python list. It can infer schema for you.\n#df1 \u003d spark.createDataFrame([(1, \"andy\", 40, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\")]).toDF(\"id\", \"name\", \"age\", \"country\")\n#df1.printSchema()\n#df1.show()\n\n# create DataFrame from pandas dataframe\n#df2 \u003d spark.createDataFrame(df1.toPandas())\n#df2.printSchema()\n#df2.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime.orderBy(\"incident_number\").show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "Offense codes"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_offense \u003d spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv(\"hdfs:///user/ubuntu/offense_codes.csv\")\ndf_offense.count()\ndf_offense.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_offense.orderBy(\"code\").show(10, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark.pyspark\nfrom pyspark.sql.functions import col\n\ndef get_cnt_of_nulls(colname):\n    return (colname + \": \" + str(df_crime.filter(col(colname).isNull()).count()))\n    \n#print(get_cnt_of_nulls(\"District\"))\n\n# use df.dtypes to get them dynamically\ncols \u003d [ \u0027INCIDENT_NUMBER\u0027,\n \u0027OFFENSE_CODE\u0027,\n \u0027OFFENSE_CODE_GROUP\u0027,\n \u0027OFFENSE_DESCRIPTION\u0027,\n \u0027DISTRICT\u0027,\n \u0027REPORTING_AREA\u0027,\n \u0027SHOOTING\u0027,\n \u0027OCCURRED_ON_DATE\u0027,\n \u0027YEAR\u0027,\n \u0027MONTH\u0027,\n \u0027DAY_OF_WEEK\u0027,\n \u0027HOUR\u0027,\n \u0027UCR_PART\u0027,\n \u0027STREET\u0027,\n \u0027LAT\u0027,\n \u0027LONG\u0027,\n \u0027LOCATION\u0027]\n \nprint(\u0027total: \u0027 + str(df_crime.count())) \nall_cols \u003d df_crime.dtypes \n\nfor c in all_cols:\n    print(get_cnt_of_nulls(c[0]))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_crime.filter(col(\"Lat\").isNull() | \n                col(\"Long\").isNull() | \n                col(\"Street\").isNull() |\n                col(\"District\").isNull() \n    ).select(\"lat\", \"long\", \"street\", \"district\").show(10, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime.groupBy(\"INCIDENT_NUMBER\").count().orderBy(col(\"count\").desc()).show(10)\n\ndf_crime.groupBy(\"INCIDENT_NUMBER\",\"OFFENSE_CODE\").count().orderBy(col(\"count\").desc()).show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime.filter(col(\"INCIDENT_NUMBER\")\u003d\u003d\"I162030584\").orderBy(\"OFFENSE_CODE\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime.filter(col(\"INCIDENT_NUMBER\")\u003d\u003d\"I152026775-00\").orderBy(\"OFFENSE_CODE\").show(truncate \u003d False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_offense.groupBy(\"CODE\").count().orderBy(col(\"count\").desc()).show(999999,truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_offense.filter((col(\"CODE\") \u003d\u003d 2608) | (col(\"CODE\") \u003d\u003d 3502)).orderBy(\"CODE\",\"NAME\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import split\n\ndf_offense.withColumn(\"CRIME_TYPE\",split(col(\"NAME\"), \u0027 - \u0027)[0]) \\\n          .filter(col(\"CODE\") \u003d\u003d 3502) \\\n          .show(20, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# !\nfrom pyspark.sql.functions import trim\n\ndf_offense_clean \u003d df_offense.withColumn(\"CRIME_TYPE\",trim(split(col(\"NAME\"), \u0027 - \u0027)[0])) \\\n                            .dropDuplicates([\"CODE\"]) \n                            \ndf_offense_clean.filter(col(\"CODE\") \u003d\u003d 3502).show()\n\n                        "
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# !\ndf_crime_clean_1 \u003d df_crime \\\n                        .join(df_offense_clean, col(\"OFFENSE_CODE\") \u003d\u003d col(\"CODE\"), \"left\") \\\n                        .select(\"INCIDENT_NUMBER\", \"CRIME_TYPE\", \"DISTRICT\", \"YEAR\",\"MONTH\", \"LAT\", \"LONG\") \\\n                        .na \\\n                        .fill(\"NA\") \n                        \n                        \ndf_crime_clean_1.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_crime_clean_1.filter(col(\"INCIDENT_NUMBER\")\u003d\u003d\"I162030584\").orderBy(\"OFFENSE_CODE\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf_crime_clean_2 \u003d df_crime_clean_1.dropDuplicates([\"INCIDENT_NUMBER\", \"CRIME_TYPE\"])\ndf_crime_clean_2.filter(col(\"INCIDENT_NUMBER\")\u003d\u003d\"I162030584\").orderBy(\"OFFENSE_CODE\").show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import countDistinct, count, sum, mean\n\n\nq1_distr \u003d df_crime_clean_2 \\\n              .groupBy(\"DISTRICT\", \"YEAR\", \"MONTH\") \\\n              .agg(countDistinct(\"INCIDENT_NUMBER\").alias(\"COUNT\")) \\\n              .groupBy(\"DISTRICT\") \\\n              .agg(sum(col(\"COUNT\")).alias(\"CRIMES_TOTAL\"), mean(col(\"COUNT\")).alias(\"CRIMES_MONTHLY\") ) \\\n              .orderBy(\"DISTRICT\")\n               \nq1_distr.show()               \n               \n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import desc, row_number, collect_list, concat_ws\n\nwindowFrequentSpec \u003d Window.partitionBy(\"DISTRICT\").orderBy(desc(\"COUNT\"))\nq2_freq_crimes \u003d df_crime_clean_2 \\\n                .groupBy(\"DISTRICT\", \"CRIME_TYPE\") \\\n                .count().alias(\"COUNT\") \\\n                .withColumn(\"ROW_NUMBER\", row_number().over(windowFrequentSpec)) \\\n                .filter(col(\"ROW_NUMBER\") \u003c\u003d 3) \\\n                .drop(\"ROW_NUMBER\") \\\n                .groupBy(\"DISTRICT\") \\\n                .agg(collect_list(\"CRIME_TYPE\").alias(\"CRIME_TYPE_ARR\")) \\\n                .withColumn(\"FREQUENT_CRIME_TYPES\", concat_ws(\",\",col(\"CRIME_TYPE_ARR\"))) \\\n                .drop(\"CRIME_TYPE_ARR\")\n\nq2_freq_crimes.show(truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n\nq3_coord \u003d df_crime_clean_2 \\\n          .filter(col(\"LAT\").isNotNull() \u0026 col(\"LONG\").isNotNull()) \\\n          .groupBy(\"DISTRICT\") \\\n          .agg(avg(\"LAT\").alias(\"lat\"),avg(\"LONG\").alias(\"lng\") )\n\n#filtering actually not needed for avg aggregates\n\nq3_coord.orderBy(\"DISTRICT\").show(999)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#q1_distr.printSchema()\n#q2_freq_crimes.printSchema()\n#q3_coord.printSchema()\n\ndf_result \u003d q1_distr \\\n            .join(q2_freq_crimes, \"DISTRICT\", \"left\") \\\n            .join(q3_coord, \"DISTRICT\", \"left\") \\\n            .orderBy(\"DISTRICT\")\n        \ndf_result.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}